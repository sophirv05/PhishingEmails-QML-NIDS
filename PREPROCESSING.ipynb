{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b877b7d",
   "metadata": {},
   "source": [
    "# Cleaning Dataset and Obtaining Samples for Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca903db",
   "metadata": {},
   "source": [
    "## a preprocessing guide for raw NIDS data for use in QML testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281ebcc",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba2057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb00839-fb36-4d6e-b5ef-e17c28a28b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dr.ivanramirez/venvs/qiskit_env/bin/python\n",
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable) \n",
    "import xgboost\n",
    "print(xgboost.__version__) \n",
    "\n",
    "import glob\n",
    "csv_files = glob.glob(\"datasets/Phishing/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e7c1a",
   "metadata": {},
   "source": [
    "### Load chosen datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45618b4-308d-4494-95e3-6419a4313f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split by label -> attack: (115181, 9) benign: (103548, 9)\n",
      "After sampling -> attack: (500, 9) benign: (500, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load chosen datasets\n",
    "# If you want to use specific files, set them here. Otherwise it will load all CSVs in datasets/Phishing/\n",
    "attack_files = [\n",
    "    \"datasets/Phishing/CEAS_08.csv\",\n",
    "    \"datasets/Phishing/Enron.csv\",\n",
    "    \"datasets/Phishing/Ling.csv\",\n",
    "    \"datasets/Phishing/Nazario.csv\",\n",
    "    \"datasets/Phishing/Nigerian_Fraud.csv\",\n",
    "    \"datasets/Phishing/phishing_email.csv\",\n",
    "    \"datasets/Phishing/SpamAssasin.csv\",\n",
    "    \"datasets/Phishing/TREC_07.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# Optionally list benign files; if empty we will split by 'label' if present in CSVs\n",
    "benign_files = []  # keep empty unless you have explicit benign CSVs\n",
    "\n",
    "# Read attack files with Dask and compute to pandas (safer for mixed schemas)\n",
    "ddf_attack = dd.read_csv(attack_files, low_memory=False)\n",
    "attack_df = ddf_attack.compute()\n",
    "attack_df[\"_source\"] = attack_df.get(\"_source_file\", \"\")  # tag source if present\n",
    "\n",
    "# If explicit benign_files provided, load them; otherwise try to split by label\n",
    "if benign_files:\n",
    "    benign_df = dd.read_csv(benign_files, low_memory=False).compute()\n",
    "else:\n",
    "    benign_df = pd.DataFrame()  # may fill below if 'label' exists\n",
    "\n",
    "# If there is a 'label' column in the combined attack_df, split on it\n",
    "if \"label\" in attack_df.columns and benign_df.empty:\n",
    "    # normalize label to 0/1\n",
    "    attack_df[\"label\"] = attack_df[\"label\"].apply(lambda v: 1 if (pd.notna(v) and int(float(v)) != 0) else 0)\n",
    "    benign_df = attack_df[attack_df[\"label\"] == 0].copy().reset_index(drop=True)\n",
    "    attack_df = attack_df[attack_df[\"label\"] == 1].copy().reset_index(drop=True)\n",
    "    print(\"split by label -> attack:\", attack_df.shape, \"benign:\", benign_df.shape)\n",
    "else:\n",
    "    print(\"Loaded attack files shape:\", attack_df.shape, \"benign explicit provided:\", not benign_df.empty)\n",
    "\n",
    "# Random sample 500 each (if available)\n",
    "n_sample = 500\n",
    "if len(attack_df) > n_sample:\n",
    "    attack_df = attack_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "if not benign_df.empty and len(benign_df) > n_sample:\n",
    "    benign_df = benign_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"After sampling -> attack:\", attack_df.shape, \"benign:\", benign_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c35f2",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed390b9-e4c2-431d-8fc8-10c3790263b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning -> attack_num: (500, 5) benign_num: (500, 5)\n"
     ]
    }
   ],
   "source": [
    "# 3. DATA CLEANING\n",
    "\n",
    "# --- Ensure we have some numeric features: if dataset is text-only, derive simple numeric features ---\n",
    "def text_to_numeric_features(df):\n",
    "    df = df.copy()\n",
    "    # safe create text-derived numeric features if columns exist\n",
    "    df[\"subject_len\"] = df.get(\"subject\", \"\").astype(str).fillna(\"\").apply(len)\n",
    "    df[\"subject_words\"] = df.get(\"subject\", \"\").astype(str).fillna(\"\").apply(lambda s: len(s.split()))\n",
    "    df[\"body_len\"] = df.get(\"body\", \"\").astype(str).fillna(\"\").apply(len)\n",
    "    df[\"body_words\"] = df.get(\"body\", \"\").astype(str).fillna(\"\").apply(lambda s: len(s.split()))\n",
    "    # count urls in body or urls column\n",
    "    df[\"body_num_urls\"] = df.get(\"body\", \"\").astype(str).fillna(\"\").apply(lambda s: len(__import__(\"re\").findall(r\"https?://|www\\.\", s)))\n",
    "    if \"urls\" in df.columns:\n",
    "        # try numeric otherwise count occurrences\n",
    "        try:\n",
    "            df[\"urls_count\"] = pd.to_numeric(df[\"urls\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        except:\n",
    "            df[\"urls_count\"] = df[\"urls\"].astype(str).fillna(\"\").apply(lambda s: len(__import__(\"re\").findall(r\"https?://|www\\.\", s)))\n",
    "    else:\n",
    "        df[\"urls_count\"] = df[\"body_num_urls\"]\n",
    "    return df\n",
    "\n",
    "attack_df = text_to_numeric_features(attack_df)\n",
    "if not benign_df.empty:\n",
    "    benign_df = text_to_numeric_features(benign_df)\n",
    "\n",
    "# Keep only numeric columns (your original intent)\n",
    "attack_num = attack_df.select_dtypes(include=[np.number]).copy()\n",
    "benign_num = benign_df.select_dtypes(include=[np.number]).copy() if not benign_df.empty else pd.DataFrame(columns=attack_num.columns)\n",
    "\n",
    "# Replace inf with NaN\n",
    "attack_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "if not benign_num.empty:\n",
    "    benign_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns with >20% missing in attack (use attack_num only)\n",
    "cols_ok = attack_num.isnull().mean() < 0.2\n",
    "cols_ok_names = cols_ok[cols_ok].index.tolist()\n",
    "attack_num = attack_num.loc[:, cols_ok_names]\n",
    "\n",
    "# Align benign to same columns if exists (use intersection to avoid boolean index misalignment)\n",
    "if not benign_num.empty:\n",
    "    shared = [c for c in cols_ok_names if c in benign_num.columns]\n",
    "    attack_num = attack_num.loc[:, shared]\n",
    "    benign_num = benign_num.loc[:, shared]\n",
    "else:\n",
    "    shared = attack_num.columns.tolist()\n",
    "\n",
    "# Drop single-value columns in attack\n",
    "multi_cols = attack_num.nunique()[attack_num.nunique() > 1].index.tolist()\n",
    "attack_num = attack_num.loc[:, multi_cols]\n",
    "if not benign_num.empty:\n",
    "    benign_num = benign_num.reindex(columns=multi_cols, fill_value=np.nan)\n",
    "\n",
    "# Drop highly collinear columns (corr > 0.95) computed on attack_num\n",
    "if attack_num.shape[1] > 1:\n",
    "    corr = attack_num.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "    if to_drop:\n",
    "        attack_num.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "        if not benign_num.empty:\n",
    "            benign_num.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Drop any column that still has NaNs: if benign exists, use benign non-null columns; otherwise drop columns with NaN in attack\n",
    "if not benign_num.empty:\n",
    "    no_nan_cols = benign_num.columns[~benign_num.isnull().any()].tolist()\n",
    "    attack_num = attack_num.loc[:, [c for c in no_nan_cols if c in attack_num.columns]]\n",
    "    benign_num = benign_num.loc[:, [c for c in no_nan_cols if c in benign_num.columns]]\n",
    "else:\n",
    "    attack_num = attack_num.dropna(axis=1, how='any')\n",
    "\n",
    "# Remove rows that are all zeros\n",
    "if not benign_num.empty and benign_num.shape[1] > 0:\n",
    "    benign_num = benign_num.loc[~(benign_num == 0.0).all(axis=1)]\n",
    "attack_num = attack_num.loc[~(attack_num == 0.0).all(axis=1)]\n",
    "\n",
    "print(\"After cleaning -> attack_num:\", attack_num.shape, \"benign_num:\", benign_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65deb6",
   "metadata": {},
   "source": [
    "## Get importances from GBF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca212d61-5318-4b7a-9aaa-bc939f413749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 features: ['body_num_urls', 'body_len', 'subject_words', 'subject_len', 'urls_count']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dr.ivanramirez/venvs/qiskit_env/lib/python3.13/site-packages/xgboost/training.py:199: UserWarning: [13:45:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# 4. Get importances from GBF model\n",
    "\n",
    "# If we have no numeric columns at all, abort\n",
    "if attack_num.shape[1] == 0:\n",
    "    raise RuntimeError(\"No numeric features available after cleaning. Check the data or relax numeric-only requirement.\")\n",
    "\n",
    "# If we have benign rows, combine and train; otherwise fallback selection\n",
    "if benign_num.empty or len(benign_num) < 2:\n",
    "    print(\"No benign data available or too few benign rows to train classifier.\")\n",
    "    # fallback: choose top 8 features by variance in attack\n",
    "    top_features = attack_num.var().nlargest(8).index.tolist()\n",
    "    print(\"Fallback top_features (by variance):\", top_features)\n",
    "else:\n",
    "    # 1. Combine and label\n",
    "    X = pd.concat([benign_num, attack_num], axis=0).reset_index(drop=True)\n",
    "    y = np.array([0] * len(benign_num) + [1] * len(attack_num))\n",
    "\n",
    "    # verify there are two classes\n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(\"Warning: labels contain only one class. Falling back to variance-based feature selection.\")\n",
    "        top_features = attack_num.var().nlargest(8).index.tolist()\n",
    "    else:\n",
    "        # 2. Train XGBoost\n",
    "        model = xgb.XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        # 3. Pick top 8 features\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "        top_features = importances.nlargest(8).index.tolist()\n",
    "        print(\"Top 8 features:\", top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51885010",
   "metadata": {},
   "source": [
    "## Construct new cleaned dataset for testing using selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453f7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 500-attack.npy (shape (500, 5))\n",
      "Saved: 500-benign.npy (shape (500, 5))\n"
     ]
    }
   ],
   "source": [
    "# 5. Construct new cleaned dataset for testing using selected features\n",
    "\n",
    "selected = [c for c in top_features if c in attack_num.columns]\n",
    "attack_final = attack_num[selected].reset_index(drop=True)\n",
    "benign_final = benign_num[selected].reset_index(drop=True) if not benign_num.empty else pd.DataFrame(columns=selected)\n",
    "\n",
    "# Save arrays\n",
    "np.save('500-attack.npy', attack_final.to_numpy())\n",
    "np.save('500-benign.npy', benign_final.to_numpy())\n",
    "\n",
    "print(\"Saved: 500-attack.npy (shape {})\".format(attack_final.shape))\n",
    "print(\"Saved: 500-benign.npy (shape {})\".format(benign_final.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56214a35-038e-4e0e-b549-1ef954389fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 20 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fc758-42ef-4927-99a9-90a92ba3ac06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qiskit_env)",
   "language": "python",
   "name": "qiskit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
