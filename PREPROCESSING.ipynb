{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b877b7d",
   "metadata": {},
   "source": [
    "# Cleaning Dataset and Obtaining Samples for Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca903db",
   "metadata": {},
   "source": [
    "## a preprocessing guide for raw NIDS data for use in QML testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f281ebcc",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba2057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb00839-fb36-4d6e-b5ef-e17c28a28b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dr.ivanramirez/venvs/qiskit_env/bin/python\n",
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable) \n",
    "import xgboost\n",
    "print(xgboost.__version__) \n",
    "\n",
    "import glob\n",
    "csv_files = glob.glob(\"datasets/Phishing/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f2ba7f-323a-4fb0-a6da-fd856570f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ spam_words: 556 urgency_words: 18 combined: 568\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_wordlist(path: str):\n",
    "    words = []\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"⚠️ Missing wordlist: {path}\")\n",
    "        return words\n",
    "\n",
    "    for line in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if s.startswith(\"#\"):\n",
    "            continue\n",
    "        words.append(s.lower())\n",
    "    return sorted(set(words))\n",
    "\n",
    "def compile_phrase_regex(phrases):\n",
    "    phrases = sorted(set([p.strip().lower() for p in phrases if p.strip()]), key=len, reverse=True)\n",
    "    escaped = []\n",
    "    for p in phrases:\n",
    "        e = re.escape(p)\n",
    "        e = e.replace(r\"\\ \", r\"\\s+\")\n",
    "        escaped.append(e)\n",
    "\n",
    "    if not escaped:\n",
    "        return None\n",
    "\n",
    "    pattern = r\"(?i)(?:\\b\" + r\"\\b|\\b\".join(escaped) + r\"\\b)\"\n",
    "    return re.compile(pattern)\n",
    "\n",
    "def count_phrase_hits(text: str, rx):\n",
    "    if rx is None:\n",
    "        return 0\n",
    "    if text is None:\n",
    "        return 0\n",
    "    s = str(text).lower()\n",
    "    return len(rx.findall(s))\n",
    "\n",
    "# Load lists once (paths are relative to QML-NIDS)\n",
    "spam_words = load_wordlist(\"wordlists/spam_words_prasidhda.txt\")\n",
    "urgency_words = load_wordlist(\"wordlists/phishing_urgency_words.txt\")\n",
    "\n",
    "combined_words = sorted(set(spam_words + urgency_words))\n",
    "\n",
    "spam_rx = compile_phrase_regex(spam_words)\n",
    "urgency_rx = compile_phrase_regex(urgency_words)\n",
    "combined_rx = compile_phrase_regex(combined_words)\n",
    "\n",
    "url_rx = re.compile(r\"https?://|www\\.\", re.IGNORECASE)\n",
    "\n",
    "print(\"✅ spam_words:\", len(spam_words), \"urgency_words:\", len(urgency_words), \"combined:\", len(combined_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e7c1a",
   "metadata": {},
   "source": [
    "### Load chosen datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45618b4-308d-4494-95e3-6419a4313f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split by label -> attack: (115181, 9) benign: (103548, 9)\n",
      "After sampling -> attack: (500, 9) benign: (500, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2. Load chosen datasets\n",
    "# If you want to use specific files, set them here. Otherwise it will load all CSVs in datasets/Phishing/\n",
    "attack_files = [\n",
    "    \"datasets/Phishing/CEAS_08.csv\",\n",
    "    \"datasets/Phishing/Enron.csv\",\n",
    "    \"datasets/Phishing/Ling.csv\",\n",
    "    \"datasets/Phishing/Nazario.csv\",\n",
    "    \"datasets/Phishing/Nigerian_Fraud.csv\",\n",
    "    \"datasets/Phishing/phishing_email.csv\",\n",
    "    \"datasets/Phishing/SpamAssasin.csv\",\n",
    "    \"datasets/Phishing/TREC_07.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# Optionally list benign files; if empty we will split by 'label' if present in CSVs\n",
    "benign_files = []  # keep empty unless you have explicit benign CSVs\n",
    "\n",
    "# Read attack files with Dask and compute to pandas (safer for mixed schemas)\n",
    "ddf_attack = dd.read_csv(attack_files, low_memory=False)\n",
    "attack_df = ddf_attack.compute()\n",
    "attack_df[\"_source\"] = attack_df.get(\"_source_file\", \"\")  # tag source if present\n",
    "\n",
    "# If explicit benign_files provided, load them; otherwise try to split by label\n",
    "if benign_files:\n",
    "    benign_df = dd.read_csv(benign_files, low_memory=False).compute()\n",
    "else:\n",
    "    benign_df = pd.DataFrame()  # may fill below if 'label' exists\n",
    "\n",
    "# If there is a 'label' column in the combined attack_df, split on it\n",
    "if \"label\" in attack_df.columns and benign_df.empty:\n",
    "    # normalize label to 0/1\n",
    "    attack_df[\"label\"] = attack_df[\"label\"].apply(lambda v: 1 if (pd.notna(v) and int(float(v)) != 0) else 0)\n",
    "    benign_df = attack_df[attack_df[\"label\"] == 0].copy().reset_index(drop=True)\n",
    "    attack_df = attack_df[attack_df[\"label\"] == 1].copy().reset_index(drop=True)\n",
    "    print(\"split by label -> attack:\", attack_df.shape, \"benign:\", benign_df.shape)\n",
    "else:\n",
    "    print(\"Loaded attack files shape:\", attack_df.shape, \"benign explicit provided:\", not benign_df.empty)\n",
    "\n",
    "# Random sample 500 each (if available)\n",
    "n_sample = 500\n",
    "if len(attack_df) > n_sample:\n",
    "    attack_df = attack_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "if not benign_df.empty and len(benign_df) > n_sample:\n",
    "    benign_df = benign_df.sample(n=n_sample, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"After sampling -> attack:\", attack_df.shape, \"benign:\", benign_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c35f2",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed390b9-e4c2-431d-8fc8-10c3790263b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Numeric features available: 24\n",
      "['label', 'urls', 'subject_len', 'subject_words', 'body_len', 'body_words', 'body_num_urls', 'urls_count', 'subject_spamword_hits', 'body_spamword_hits', 'subject_urgency_hits', 'body_urgency_hits', 'subject_keyword_hits', 'body_keyword_hits', 'subject_exclaim', 'body_exclaim', 'subject_qmark', 'body_qmark', 'subject_digits', 'body_digits', 'subject_upper_ratio', 'body_upper_ratio', 'subject_money', 'body_money']\n",
      "After cleaning -> attack_num: (500, 19) benign_num: (500, 19)\n"
     ]
    }
   ],
   "source": [
    "# 3. DATA CLEANING\n",
    "\n",
    "def text_to_numeric_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    subj = df.get(\"subject\", \"\").astype(str).fillna(\"\")\n",
    "    body = df.get(\"body\", \"\").astype(str).fillna(\"\")\n",
    "\n",
    "    # Length + word count\n",
    "    df[\"subject_len\"] = subj.apply(len)\n",
    "    df[\"subject_words\"] = subj.apply(lambda s: len(s.split()))\n",
    "    df[\"body_len\"] = body.apply(len)\n",
    "    df[\"body_words\"] = body.apply(lambda s: len(s.split()))\n",
    "\n",
    "    # URLs\n",
    "    df[\"body_num_urls\"] = body.apply(lambda s: len(url_rx.findall(s)))\n",
    "    if \"urls\" in df.columns:\n",
    "        df[\"urls_count\"] = pd.to_numeric(df[\"urls\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    else:\n",
    "        df[\"urls_count\"] = df[\"body_num_urls\"]\n",
    "\n",
    "    # NEW: spam/phishing keyword metrics (Mary + Brian request)\n",
    "    df[\"subject_spamword_hits\"] = subj.apply(lambda s: count_phrase_hits(s, spam_rx))\n",
    "    df[\"body_spamword_hits\"] = body.apply(lambda s: count_phrase_hits(s, spam_rx))\n",
    "\n",
    "    df[\"subject_urgency_hits\"] = subj.apply(lambda s: count_phrase_hits(s, urgency_rx))\n",
    "    df[\"body_urgency_hits\"] = body.apply(lambda s: count_phrase_hits(s, urgency_rx))\n",
    "\n",
    "    df[\"subject_keyword_hits\"] = subj.apply(lambda s: count_phrase_hits(s, combined_rx))\n",
    "    df[\"body_keyword_hits\"] = body.apply(lambda s: count_phrase_hits(s, combined_rx))\n",
    "\n",
    "    # Extra simple phishing signals\n",
    "    df[\"subject_exclaim\"] = subj.apply(lambda s: s.count(\"!\"))\n",
    "    df[\"body_exclaim\"] = body.apply(lambda s: s.count(\"!\"))\n",
    "\n",
    "    df[\"subject_qmark\"] = subj.apply(lambda s: s.count(\"?\"))\n",
    "    df[\"body_qmark\"] = body.apply(lambda s: s.count(\"?\"))\n",
    "\n",
    "    df[\"subject_digits\"] = subj.apply(lambda s: sum(ch.isdigit() for ch in s))\n",
    "    df[\"body_digits\"] = body.apply(lambda s: sum(ch.isdigit() for ch in s))\n",
    "\n",
    "    df[\"subject_upper_ratio\"] = subj.apply(lambda s: (sum(ch.isupper() for ch in s) / max(len(s), 1)))\n",
    "    df[\"body_upper_ratio\"] = body.apply(lambda s: (sum(ch.isupper() for ch in s) / max(len(s), 1)))\n",
    "\n",
    "    df[\"subject_money\"] = subj.apply(lambda s: s.count(\"$\") + s.lower().count(\"usd\"))\n",
    "    df[\"body_money\"] = body.apply(lambda s: s.count(\"$\") + s.lower().count(\"usd\"))\n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "attack_df = text_to_numeric_features(attack_df)\n",
    "if not benign_df.empty:\n",
    "    benign_df = text_to_numeric_features(benign_df)\n",
    "\n",
    "attack_num_preview = attack_df.select_dtypes(include=[np.number])\n",
    "print(\"✅ Numeric features available:\", attack_num_preview.shape[1])\n",
    "print(attack_num_preview.columns.tolist())\n",
    "attack_num_preview.head() \n",
    "\n",
    "# Keep only numeric columns (your original intent)\n",
    "attack_num = attack_df.select_dtypes(include=[np.number]).copy()\n",
    "benign_num = benign_df.select_dtypes(include=[np.number]).copy() if not benign_df.empty else pd.DataFrame(columns=attack_num.columns)\n",
    "\n",
    "# Drop label + urls so they never become features (prevents leakage / weird encoding)\n",
    "attack_num = attack_num.drop(columns=[\"label\", \"urls\"], errors=\"ignore\")\n",
    "if not benign_num.empty:\n",
    "    benign_num = benign_num.drop(columns=[\"label\", \"urls\"], errors=\"ignore\")\n",
    "\n",
    "# Replace inf with NaN\n",
    "attack_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "if not benign_num.empty:\n",
    "    benign_num.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop columns with >20% missing in attack (use attack_num only)\n",
    "cols_ok = attack_num.isnull().mean() < 0.2\n",
    "cols_ok_names = cols_ok[cols_ok].index.tolist()\n",
    "attack_num = attack_num.loc[:, cols_ok_names]\n",
    "\n",
    "# Align benign to same columns if exists (use intersection to avoid boolean index misalignment)\n",
    "if not benign_num.empty:\n",
    "    shared = [c for c in cols_ok_names if c in benign_num.columns]\n",
    "    attack_num = attack_num.loc[:, shared]\n",
    "    benign_num = benign_num.loc[:, shared]\n",
    "else:\n",
    "    shared = attack_num.columns.tolist()\n",
    "\n",
    "# Drop single-value columns in attack\n",
    "multi_cols = attack_num.nunique()[attack_num.nunique() > 1].index.tolist()\n",
    "attack_num = attack_num.loc[:, multi_cols]\n",
    "if not benign_num.empty:\n",
    "    benign_num = benign_num.reindex(columns=multi_cols, fill_value=np.nan)\n",
    "\n",
    "# Drop highly collinear columns (corr > 0.95) computed on attack_num\n",
    "if attack_num.shape[1] > 1:\n",
    "    corr = attack_num.corr().abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\n",
    "    if to_drop:\n",
    "        attack_num.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "        if not benign_num.empty:\n",
    "            benign_num.drop(columns=to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Instead of dropping columns (too aggressive), fill missing values\n",
    "attack_num = attack_num.fillna(0)\n",
    "if not benign_num.empty:\n",
    "    benign_num = benign_num.fillna(0)\n",
    "\n",
    "# Remove rows that are all zeros\n",
    "if not benign_num.empty and benign_num.shape[1] > 0:\n",
    "    benign_num = benign_num.loc[~(benign_num == 0.0).all(axis=1)]\n",
    "attack_num = attack_num.loc[~(attack_num == 0.0).all(axis=1)]\n",
    "\n",
    "print(\"After cleaning -> attack_num:\", attack_num.shape, \"benign_num:\", benign_num.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65deb6",
   "metadata": {},
   "source": [
    "## Get importances from GBF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca212d61-5318-4b7a-9aaa-bc939f413749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 features: ['body_num_urls', 'body_exclaim', 'subject_spamword_hits', 'body_len', 'body_digits', 'body_spamword_hits', 'subject_upper_ratio', 'subject_digits']\n",
      "Top features selected: ['body_num_urls', 'body_exclaim', 'subject_spamword_hits', 'body_len', 'body_digits', 'body_spamword_hits', 'subject_upper_ratio', 'subject_digits']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dr.ivanramirez/venvs/qiskit_env/lib/python3.13/site-packages/xgboost/training.py:199: UserWarning: [20:38:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# 4. Get importances from GBF model\n",
    "\n",
    "# If we have no numeric columns at all, abort\n",
    "if attack_num.shape[1] == 0:\n",
    "    raise RuntimeError(\"No numeric features available after cleaning. Check the data or relax numeric-only requirement.\")\n",
    "\n",
    "# If we have benign rows, combine and train; otherwise fallback selection\n",
    "if benign_num.empty or len(benign_num) < 2:\n",
    "    print(\"No benign data available or too few benign rows to train classifier.\")\n",
    "    # fallback: choose top 8 features by variance in attack\n",
    "    top_features = attack_num.var().nlargest(8).index.tolist()\n",
    "    print(\"Fallback top_features (by variance):\", top_features)\n",
    "else:\n",
    "    # 1. Combine and label\n",
    "    X = pd.concat([benign_num, attack_num], axis=0).reset_index(drop=True)\n",
    "    y = np.array([0] * len(benign_num) + [1] * len(attack_num))\n",
    "\n",
    "    # verify there are two classes\n",
    "    if len(np.unique(y)) < 2:\n",
    "        print(\"Warning: labels contain only one class. Falling back to variance-based feature selection.\")\n",
    "        top_features = attack_num.var().nlargest(8).index.tolist()\n",
    "    else:\n",
    "        # 2. Train XGBoost\n",
    "        model = xgb.XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=100,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        model.fit(X, y)\n",
    "        # 3. Pick top 8 features\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "        top_features = importances.nlargest(8).index.tolist()\n",
    "        print(\"Top 8 features:\", top_features)\n",
    "        print(\"Top features selected:\", top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51885010",
   "metadata": {},
   "source": [
    "## Construct new cleaned dataset for testing using selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453f7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 500-attack.npy (shape (500, 8))\n",
      "Saved: 500-benign.npy (shape (500, 8))\n"
     ]
    }
   ],
   "source": [
    "# 5. Construct new cleaned dataset for testing using selected features\n",
    "\n",
    "selected = [c for c in top_features if c in attack_num.columns]\n",
    "attack_final = attack_num[selected].reset_index(drop=True)\n",
    "benign_final = benign_num[selected].reset_index(drop=True) if not benign_num.empty else pd.DataFrame(columns=selected)\n",
    "\n",
    "# Save arrays\n",
    "np.save('500-attack.npy', attack_final.to_numpy())\n",
    "np.save('500-benign.npy', benign_final.to_numpy())\n",
    "\n",
    "print(\"Saved: 500-attack.npy (shape {})\".format(attack_final.shape))\n",
    "print(\"Saved: 500-benign.npy (shape {})\".format(benign_final.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fc758-42ef-4927-99a9-90a92ba3ac06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qiskit_env)",
   "language": "python",
   "name": "qiskit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
